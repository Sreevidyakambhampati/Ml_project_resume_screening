{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "import nltk\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load dataset\n",
    "file_path = \"/content/resume_dataset (1).csv\"  # Change to the actual path if needed\n",
    "df = pd.read_csv(file_path, encoding=\"utf-8\")\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Show unique categories\n",
    "print(\"\\nUnique Resume Categories:\\n\", df[\"Category\"].unique())\n",
    "\n",
    "# Count plot of categories\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(y=\"Category\", data=df, order=df[\"Category\"].value_counts().index)\n",
    "plt.title(\"Resume Category Distribution\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Category\")\n",
    "plt.show()\n",
    "\n",
    "# Text Cleaning Function\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)  # Remove URLs\n",
    "    text = re.sub(r\"[^a-zA-Z]\", \" \", text)  # Remove non-alphabetic characters\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Apply text cleaning\n",
    "df[\"cleaned_resume\"] = df[\"Resume\"].apply(clean_text)\n",
    "\n",
    "# Word Cloud\n",
    "text_data = \" \".join(df[\"cleaned_resume\"])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text_data)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "df[\"Category\"] = le.fit_transform(df[\"Category\"])\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=1500)\n",
    "X = vectorizer.fit_transform(df[\"cleaned_resume\"])\n",
    "y = df[\"Category\"]\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest Model\n",
    "rf_classifier = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make Predictions\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf, target_names=le.classes_))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt', download_dir='/usr/local/nltk_data')\n",
    "nltk.download('stopwords', download_dir='/usr/local/nltk_data')\n",
    "\n",
    "# Set the correct path for nltk data\n",
    "nltk.data.path.append('/usr/local/nltk_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load dataset (update path if needed)\n",
    "file_path = \"/content/resume_dataset (1).csv\"  # Update this if necessary\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check dataset structure\n",
    "print(\"Columns in dataset:\", df.columns)\n",
    "print(\"Missing values in Resume column:\", df['Resume'].isnull().sum())\n",
    "\n",
    "# Fill any missing values\n",
    "df['Resume'].fillna(\"\", inplace=True)\n",
    "\n",
    "# Convert to string (ensures no type issues)\n",
    "df['Resume'] = df['Resume'].astype(str)\n",
    "\n",
    "# Display sample data\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    tokens = text.split()  # Simple tokenization (avoids NLTK issues)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Apply text preprocessing\n",
    "df['cleaned_text'] = df['Resume'].apply(preprocess_text)\n",
    "\n",
    "# Display processed data sample\n",
    "print(df[['Resume', 'cleaned_text']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training (80%) and testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['Category'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Print sizes of the datasets\n",
    "print(\"Training samples:\", len(X_train))\n",
    "print(\"Testing samples:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert text into TF-IDF features\n",
    "tfidf = TfidfVectorizer(max_features=5000)  # Use top 5000 important words\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "# Print shape of transformed data\n",
    "print(\"TF-IDF Train Shape:\", X_train_tfidf.shape)\n",
    "print(\"TF-IDF Test Shape:\", X_test_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the TF-IDF feature set\n",
    "rf_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = rf_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Random Forest Model Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
    "class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "\n",
    "# Define improved LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(10000, 256, input_length=200),  # Increased embedding size\n",
    "    SpatialDropout1D(0.3),\n",
    "    LSTM(150, dropout=0.3, recurrent_dropout=0.3, return_sequences=True),\n",
    "    LSTM(100, dropout=0.3, recurrent_dropout=0.3),\n",
    "    Dense(len(label_encoder.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with class weights\n",
    "lstm_model.fit(X_train_pad, y_train_encoded, epochs=20, batch_size=32, validation_data=(X_test_pad, y_test_encoded), class_weight=class_weights_dict)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred_lstm = lstm_model.predict(X_test_pad)\n",
    "y_pred_lstm_classes = y_pred_lstm.argmax(axis=1)\n",
    "y_pred_lstm_labels = label_encoder.inverse_transform(y_pred_lstm_classes)\n",
    "\n",
    "# Accuracy & Report\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "print(\"Improved LSTM Accuracy:\", accuracy_score(y_test, y_pred_lstm_labels))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_lstm_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries (Run this in Colab)\n",
    "!pip install transformers datasets\n",
    "\n",
    "# Import Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/resume_dataset.csv')\n",
    "\n",
    "# Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['Category_Label'] = label_encoder.fit_transform(df['Category'])\n",
    "\n",
    "# Split Data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Resume'], df['Category_Label'], test_size=0.2, stratify=df['Category_Label'])\n",
    "\n",
    "# Load BERT Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize Texts\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "train_encodings = tokenize_function(train_texts.tolist())\n",
    "test_encodings = tokenize_function(test_texts.tolist())\n",
    "\n",
    "# Convert to Dataset Format\n",
    "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask'], 'labels': train_labels.tolist()})\n",
    "test_dataset = Dataset.from_dict({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask'], 'labels': test_labels.tolist()})\n",
    "\n",
    "# Load Pretrained BERT Model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate Model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Convert Predictions to Labels\n",
    "y_pred_labels = label_encoder.inverse_transform(preds)\n",
    "y_true_labels = label_encoder.inverse_transform(test_labels)\n",
    "\n",
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"BERT Model Classification Report:\\n\", classification_report(y_true_labels, y_pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install necessary libraries (Run this in Colab)\n",
    "!pip install transformers datasets\n",
    "\n",
    "# Import Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/resume_dataset.csv')\n",
    "\n",
    "# Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['Category_Label'] = label_encoder.fit_transform(df['Category'])\n",
    "\n",
    "# Split Data\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Resume'], df['Category_Label'], test_size=0.2, stratify=df['Category_Label'])\n",
    "\n",
    "# Load BERT Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize Texts\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=512)\n",
    "\n",
    "train_encodings = tokenize_function(train_texts.tolist())\n",
    "test_encodings = tokenize_function(test_texts.tolist())\n",
    "\n",
    "# Convert to Dataset Format\n",
    "train_dataset = Dataset.from_dict({'input_ids': train_encodings['input_ids'], 'attention_mask': train_encodings['attention_mask'], 'labels': train_labels.tolist()})\n",
    "test_dataset = Dataset.from_dict({'input_ids': test_encodings['input_ids'], 'attention_mask': test_encodings['attention_mask'], 'labels': test_labels.tolist()})\n",
    "\n",
    "# Load Pretrained BERT Model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# Train Model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate Model\n",
    "trainer.evaluate()\n",
    "\n",
    "# Predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "# Convert Predictions to Labels\n",
    "y_pred_labels = label_encoder.inverse_transform(preds)\n",
    "y_true_labels = label_encoder.inverse_transform(test_labels)\n",
    "\n",
    "# Classification Report\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"BERT Model Classification Report:\\n\", classification_report(y_true_labels, y_pred_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/content/drive/MyDrive/resume_dataset.csv')\n",
    "\n",
    "# Encode labels\n",
    "df['Category'] = df['Category'].astype('category').cat.codes\n",
    "\n",
    "# Split dataset\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df['Resume'].tolist(), df['Category'].tolist(), test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize data\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_dict({\"input_ids\": train_encodings[\"input_ids\"], \"labels\": train_labels})\n",
    "val_dataset = Dataset.from_dict({\"input_ids\": val_encodings[\"input_ids\"], \"labels\": val_labels})\n",
    "\n",
    "# Load Pretrained Model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=df['Category'].nunique())\n",
    "model.to(device)\n",
    "\n",
    "# Training Arguments (Faster Execution)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,         # Reduce epochs for speed\n",
    "    per_device_train_batch_size=8,  # Lower batch size to reduce memory usage\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/resume_dataset.csv\")\n",
    "\n",
    "# Encode labels\n",
    "df[\"Category\"], category_labels = pd.factorize(df[\"Category\"])\n",
    "\n",
    "# Split dataset\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"Resume\"], df[\"Category\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize inputs with attention masks\n",
    "def tokenize_data(texts, labels):\n",
    "    encodings = tokenizer(\n",
    "        list(texts), truncation=True, padding=True, max_length=512, return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": encodings[\"input_ids\"],\n",
    "        \"attention_mask\": encodings[\"attention_mask\"],\n",
    "        \"labels\": torch.tensor(labels.values)\n",
    "    }\n",
    "\n",
    "train_dataset = tokenize_data(train_texts, train_labels)\n",
    "val_dataset = tokenize_data(val_texts, val_labels)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "val_dataset = Dataset.from_dict(val_dataset)\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(category_labels))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"/content/drive/MyDrive/bert_resume_classifier\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/bert_resume_classifier\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/resume_dataset.csv\")\n",
    "\n",
    "# Encode labels\n",
    "df[\"Category\"], category_labels = pd.factorize(df[\"Category\"])\n",
    "\n",
    "# Split dataset\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df[\"Resume\"], df[\"Category\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize inputs with attention masks\n",
    "def tokenize_data(texts, labels):\n",
    "    encodings = tokenizer(\n",
    "        list(texts), truncation=True, padding=True, max_length=512, return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"input_ids\": encodings[\"input_ids\"],\n",
    "        \"attention_mask\": encodings[\"attention_mask\"],\n",
    "        \"labels\": torch.tensor(labels.values)\n",
    "    }\n",
    "\n",
    "train_dataset = tokenize_data(train_texts, train_labels)\n",
    "val_dataset = tokenize_data(val_texts, val_labels)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_dict(train_dataset)\n",
    "val_dataset = Dataset.from_dict(val_dataset)\n",
    "\n",
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(category_labels))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate model\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"/content/drive/MyDrive/bert_resume_classifier\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/bert_resume_classifier\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
