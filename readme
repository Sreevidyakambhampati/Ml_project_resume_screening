# üìÑ Resume Screening and Classification System

This project demonstrates the complete pipeline for an **AI-based Resume Classification System** using traditional Machine Learning models (Random Forest, Naive Bayes), Deep Learning (LSTM), and Transformer-based models (BERT, DistilBERT). The system classifies resumes into predefined job categories such as Data Science, Web Development, HR, etc.

---

## üîç Problem Statement

Manually reviewing resumes is time-consuming and error-prone. This project automates the classification of resumes into job-related categories using machine learning and deep learning techniques, helping HR professionals short-list suitable candidates faster and more accurately.

---

## üìÅ Dataset

* **Name:** `resume_dataset.csv`
* **Columns:**

  * `Resume`: Raw resume text.
  * `Category`: Job category label (e.g., Data Science, HR, DevOps, etc.).

> üìå Note: Dataset is not publicly shared due to licensing. Replace `/content/drive/...` with your local or cloud path as needed.

---

## üîß Project Structure

```
‚îú‚îÄ‚îÄ preprocessing/
‚îÇ   ‚îî‚îÄ‚îÄ text_cleaning.py
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ random_forest.py
‚îÇ   ‚îú‚îÄ‚îÄ naive_bayes.py
‚îÇ   ‚îú‚îÄ‚îÄ lstm_model.py
‚îÇ   ‚îú‚îÄ‚îÄ bert_model.py
‚îÇ   ‚îî‚îÄ‚îÄ distilbert_model.py
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ metrics.py
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ resume_dataset.csv
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt
```

---

## ‚öôÔ∏è Technologies & Libraries

* **NLP**: NLTK, WordCloud, re
* **Traditional ML**: Scikit-learn (Random Forest, Naive Bayes, KNN)
* **Deep Learning**: TensorFlow, Keras
* **Transformers**: Hugging Face (`transformers`, `datasets`)
* **Visualization**: Matplotlib, Seaborn
* **Language**: Python 3.x

---

## üìä Models Implemented

### ‚úÖ 1. Traditional Machine Learning

* TF-IDF Vectorization
* Models:

  * Random Forest
  * Naive Bayes
* Achieved baseline accuracy \~85‚Äì90%

### ‚úÖ 2. Deep Learning ‚Äì LSTM

* Tokenization + Padding
* LSTM architecture with dropout layers
* Class weights for imbalance handling
* Achieved accuracy up to \~92%

### ‚úÖ 3. Transformer-based Models

* **BERT** (`bert-base-uncased`)
* **DistilBERT** (faster training)
* Tokenization using HuggingFace `Tokenizer`
* Fine-tuned using HuggingFace `Trainer` API
* Best performance with \~94‚Äì96% accuracy

---

## üìà Evaluation Metrics

* Accuracy Score
* Classification Report (Precision, Recall, F1-Score)
* Evaluation done using `sklearn.metrics`

---

## üìå How to Run

### üõ†Ô∏è Install Dependencies

```bash
pip install -r requirements.txt
```

> Contents of `requirements.txt`:

```txt
pandas
numpy
matplotlib
seaborn
nltk
scikit-learn
wordcloud
tensorflow
transformers
datasets
```

### ‚ñ∂Ô∏è Run Models

#### 1. Traditional ML

```bash
python models/random_forest.py
```

#### 2. LSTM Model

```bash
python models/lstm_model.py
```

#### 3. BERT / DistilBERT

```bash
python models/bert_model.py
# or
python models/distilbert_model.py
```

---

## üì¶ Model Saving & Deployment

* BERT model saved at:
  `/content/drive/MyDrive/bert_resume_classifier/`

* Can be exported to TensorFlow Lite or ONNX for integration in production systems.

---

## üìâ Visualizations

* WordCloud for common keywords across resumes
* Category distribution using `seaborn` countplot
* Confusion matrices and classification metrics

---

## üß† Future Enhancements

* Resume ranking based on JD match %
* Integration with ATS (Applicant Tracking Systems)
* Deploy via Flask or FastAPI for real-time prediction
